{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType,DoubleType,BooleanType,DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"client_id\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": 'client_secret_key',\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/client_tenant_id/oauth2/token\"\n",
    "}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=\"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net\",  #contrainer@storageacc\n",
    "    mount_point=\"/mnt/supplychain\",\n",
    "    extra_configs=configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/supplychain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/mnt/supplychain/raw_data/sales_data.csv\")\n",
    "inventory_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/mnt/supplychain/raw_data/inventory_data.csv\")\n",
    "supplier_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/mnt/supplychain/raw_data/supplier_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic commands to check your dataset and schema\n",
    "sales_data.show()\n",
    "sales_data.printSchema()\n",
    "inventory_data.show()\n",
    "inventory_data.printSchema()\n",
    "supplier_data.show()\n",
    "supplier_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "sales_data = sales_data.dropDuplicates()\n",
    "inventory_data = inventory_data.dropDuplicates()\n",
    "supplier_data = supplier_data.dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "sales_data=sales_data.drop(\"Row ID\",\"Ship Mode\",\"Region\",\"Sub-Category\" )\n",
    "supplier_data=supplier_data.drop(\"SKU\",\"Customer demographics\",\"Shipping carriers\",\"Inspection results\",\"Routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organising data\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Categorize products based on sales performance\n",
    "sales_data = sales_data.withColumn(\"Performance_category\",\n",
    "    when(sales_data[\"Sales\"] > 100, \"High Performer\")\n",
    "    .when(sales_data[\"Sales\"] > 50, \"Medium Performer\")\n",
    "    .otherwise(\"Low Performer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a column for stock level status\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "inventory_data = inventory_data.withColumn(\"Stock_status\",\n",
    "    when(inventory_data[\"Quantity\"] < 10, \"Low Stock\")\n",
    "    .when(inventory_data[\"Quantity\"] > 100, \"Excess Stock\")\n",
    "    .otherwise(\"Normal Stock\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define a window specification based on product_id\n",
    "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"StockCode\")\n",
    "\n",
    "# Add a row number to each entry\n",
    "inventory_data = inventory_data.withColumn(\"Row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Use the row number to calculate lag\n",
    "inventory_data = inventory_data.withColumn(\"Previous_stock_level\", lag(\"Quantity\").over(window_spec))\n",
    "\n",
    "# Calculate the difference\n",
    "inventory_data = inventory_data.withColumn(\"Stock_change\", inventory_data[\"Quantity\"] - inventory_data[\"Previous_stock_level\"])\n",
    "\n",
    "inventory_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import random\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Define the start and end date range\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2023, 1, 1)\n",
    "\n",
    "# Create a UDF to generate random dates\n",
    "def random_date():\n",
    "    return start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
    "\n",
    "random_date_udf = F.udf(random_date, DateType())\n",
    "\n",
    "# Replace NULL dates with random dates\n",
    "inventory_data = inventory_data.withColumn(\n",
    "    \"InvoiceDate\", \n",
    "    F.when(F.col(\"InvoiceDate\").isNull(), random_date_udf()).otherwise(F.col(\"InvoiceDate\"))\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "inventory_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Parquet file\n",
    "sales_data.repartition(1).write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"/mnt/supplychain/transformed_data/sales_data.parquet\")\n",
    "inventory_data.repartition(1).write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"/mnt/supplychain/transformed_data/inventory_data.parquet\")\n",
    "supplier_data.repartition(1).write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"/mnt/supplychain/transformed_data/supplier_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformeddata in our synapse workspace\n",
    "inventory_data = spark.read.parquet(\"abfss://supply-chain-data@supplychainstoragek.dfs.core.windows.net/transformed_data/inventory_data.parquet\")\n",
    "supplier_data = spark.read.parquet(\"abfss://supply-chain-data@supplychainstoragek.dfs.core.windows.net/transformed_data/supplier_data.parquet\")\n",
    "sales_data = spark.read.parquet(\"abfss://supply-chain-data@supplychainstoragek.dfs.core.windows.net/transformed_data/sales_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"StockCode\").orderBy(\"Invoicedate\").rowsBetween(-3, 0)\n",
    "\n",
    "# Create a rolling average feature\n",
    "inventory_data = inventory_data.withColumn(\"Rolling_avg_stock\", avg(\"Quantity\").over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Make sure the column name is exactly as it appears in your DataFrame\n",
    "feature_cols = [\"Rolling_avg_stock\"]\n",
    "\n",
    "# Initialize the VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "inventory_data = assembler.transform(inventory_data)\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "inventory_data.select(\"features\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data, test_data = inventory_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Check the number of rows in each set\n",
    "print(f\"Training Data Count: {train_data.count()}\")\n",
    "print(f\"Test Data Count: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data, test_data = inventory_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Check the number of rows in each set\n",
    "print(f\"Training Data Count: {train_data.count()}\")\n",
    "print(f\"Test Data Count: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Quantity\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Print out the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_results = lr_model.transform(test_data)\n",
    "\n",
    "# Initialize the regression evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"Quantity\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "rmse = evaluator.evaluate(test_results)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "lr_model.save(\"/mnt/supplychain/models/inventory_forecast_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = LinearRegressionModel.load(\"/mnt/supplychain/models/inventory_forecast_model\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "new_predictions = loaded_model.transform(test_data)\n",
    "new_predictions.select(\"features\", \"Quantity\", \"prediction\").show()\n",
    "\n",
    "\n",
    "# Sample output\n",
    "#+--------+--------+--------------------+\n",
    "|features|Quantity|          prediction|\n",
    "+--------+--------+--------------------+\n",
    "|  [14.5]|       6|  14.715033842772941|\n",
    "|   [2.0]|       3|  1.0593837828082833|\n",
    "|   [8.0]|       2|   7.614095811591319|\n",
    "|   [4.0]|       1|  3.2442877924026288|\n",
    "|   [5.5]|       4|   4.882965799598388|\n",
    "|  [0.75]|       2| -0.3061812231881825|\n",
    "| [12.25]|      40|  12.257016831979303|\n",
    "|   [5.5]|       4|   4.882965799598388|\n",
    "|  [28.5]|      96|  30.009361909933357|\n",
    "|  [27.5]|      96|  28.916909905136187|\n",
    "|  [3.25]|       4|   2.424948788804749|\n",
    "|  [28.5]|       6|  30.009361909933357|\n",
    "|  [13.5]|       9|  13.622581837975769|\n",
    "|   [1.0]|       1|-0.03306822198888937|\n",
    "|   [6.5]|       8|    5.97541780439556|\n",
    "|  [28.5]|       6|  30.009361909933357|\n",
    "|  [28.0]|      28|  29.463135907534774|\n",
    "| [136.0]|      24|  147.44795242562944|\n",
    "|   [1.0]|       1|-0.03306822198888937|\n",
    "| [11.25]|       5|  11.164564827182131|\n",
    "+--------+--------+--------------------+\n",
    "only showing top 20 rows"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
